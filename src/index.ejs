<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style>
.eq-grid {
  padding: 0px;
  padding-left: 0px;
  display: grid;
  justify-content: center;
  align-items: center;
  grid-row-gap: 0px;
  margin-top: 0px;
  margin-bottom: 20px;
}
.eq-grid d-math {
  font-size: 120%;
}
.eq-grid figcaption d-math {
  font-size: 100%;
}
.eq-grid figcaption {
}
.eq-grid .expansion-marker {
  border: 1px solid #CCC;
  border-bottom: none;
  height: 6px;
  width: 100%;
  margin-top: 6px;
  margin-bottom: 6px;
}
.eq-grid .faded {
  opacity: 0.5;
}

.option {
  font-size: 16px;
}
  </style>
}
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Some Examples of Useful, Non-Robust Features",
  "description": "An example project using webpack and svelte-loader and ejs to inline SVGs",
  "password": "svgs",
  "authors": [
    {
      "author": "Ludwig Schubert",
      "authorURL": "https://schubert.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title></d-title>


<d-article>

  <p>
    A <i>feature</i>, as defined by Ilyas et al, is a function $f$ that takes the model inputs of the <i>data distribution</i> $(x,y) \sim \mathcal{D}$ into a real number, restricted to have mean zero and unit variance. A feature is said to be <i>useful</i> if it has high correlation with the label. In the presense of an adversery Ilyas et al. argues the metric that truly matters is a feature's <i>robust useflness</i>,
    
    <center>
    <div class="eq-grid">
    <d-math block>\mathbf{E}\left[\inf_{\|\delta\|\leq\epsilon}yf(x+\delta)\right]
    </d-math>
    </div>
    </center>

    its correlation with the label while under attack. 
  </p>

  <p>
    One might suspect that the most pedestrian features, such as the color of the sky, or the presense of a floppy ear on a dog, are robust and useful --- but our present statistical models, Ilias suggests, may be taking advantage of useful, non-robust features, some of which may even lie beyond the threshold of human intuition. This begs the question: what might such non-robust features look like? 
  </p>

  <h3>Non-Robust Features in Linear Models</h3>

  <p>
    The search for such a non-robust feature is considerabily simplified when we realize we the phenomenan of non-robust features are not unique to complex, non-linear models. Thus, we restrict our attention to linear features of the form

    <center>
    <div class="eq-grid">
    <d-math block>f(x) = \frac{a^Tx}{\|a\|_\Sigma}\qquad \text{where} \qquad \Sigma = \mathbf{E}[xx^T],
    </d-math>
    </div>
    </center>

    and assume, for notational simplicity, that $\mathbf{E}[x] = 0$. Here, the notation of robust usefuless admits following decomposition <d-footnote> This
     <d-math block> 
      \begin{aligned}
      \mathbf{E}\left[\inf_{\|\delta\|\leq\epsilon}yf(x+\delta)\right] & =\mathbf{E}\left[yf(x)+\inf_{\|\delta\|\leq\epsilon}yf(\delta)\right]\\
       & =\mathbf{E}\left[yf(x)+\inf_{\|\delta\|\leq\epsilon}y\frac{a^{T}\delta}{\|a\|_{\Sigma}}\right]\\
       & =\mathbf{E}\left[yf(x)+\frac{\inf_{\|\delta\|\leq\epsilon}a^{T}\delta}{\|a\|_{\Sigma}}\right]=\mathop{\mathbf{E}[yf(x)]}-\epsilon\frac{\|a\|_{*}}{\|a\|_{\Sigma}}
      \end{aligned}
    </d-math>
    </d-footnote>

  </p>
  <div class="eq-grid">

    <div style="grid-row: 1; grid-column: 1; max-width: 170px"><d-math block>\mathbf{E}\left[\inf_{\|\delta\|\leq\epsilon}yf(x+\delta)\right]
   =</d-math></div>
    <figcaption style="grid-row: 2; grid-column: 1;">
      A linear feature's robust usefulness breaks into two terms:
    </figcaption>

    <div style="grid-row: 1; grid-column: 2; align-items: center" >
      <d-math  block>\mathop{\mathbf{E}[yf(x)]}</d-math>
    </div>
    <figcaption style="grid-row: 2; grid-column: 2;">
      One equal to the correlation of $f(x)$ with label, it's usefulness,
    </figcaption>

    <div style="grid-row: 1; grid-column: 3;" >
      <d-math  block><center>-</center></d-math>
    </div>

    <div style="grid-row: 1; grid-column: 4;">
      <d-math block>\epsilon\frac{\|a\|_{*}}{\|a\|_{\Sigma}}</d-math>
    </div>
    <figcaption style="grid-row: 2; grid-column: 4;">
      and the second term measuring the metric misalignment between the adversery and the
    </figcaption>

  </div>

  <p>
    Where $\|\cdot\|_*$ deontes the dual norm of $\|\cdot\|$. 
  </p>
  <p>
    This decomposition gives us an instrument for visualizing the space of linear features. Given the binary classification task of seperating
    <select name="cars" style="font-size:16px">
    <option value="volvo">"truck" and "airplane"
    </option> 
    <option value="volvo">4 from 7
    </option>
    </select>
    in 
    <select name="cars" style="font-size:16px">
    <option>CIFAR-10</option>
    <option value="saab">non-Robust dataset</option>
    <option value="saab">robust dataset</option>
    <option>MNIST</option>
    </select>, we can emprically estimate the correlation and plot all the features  
    <select name="cars" style="font-size:16px">
    <option value="volvo">left singular vectors of the data</option>
    <option value="saab">pixels</option>
    <option value="saab">random directions</option>
    </select>
    in a two dimensional plot:
  </p>

  <figure>
    <div id="what"></div>
    <p>
    <center>
    <figcaption>asda</figcaption>
    </center>
  </p>
  </figure>  

  <h3> Ensembles and Distractors </h3>

  <p>
    The work of Tsiprasâˆ— et al. suggests a simple construction --- a collection of non-robust and non-useful features, if sufficiently uncorrelated, can be ensembled into a useful, non-robust feature.
  </p>

  <figure>
    <div id="what2"></div>
    <p>
    <center>
    <figcaption>asda</figcaption>
    </center>
  </p>
  </figure>  

  <p>
    Consider now an alternative construction. Let us consider what happens when we take a robust, useful feature and interpolate it with a non-robust, useless feature.
  </p>

  <figure>
    <div id="what3"></div>
    <p>
    <center>
    <figcaption>asda</figcaption>
    </center>
  </p>
  </figure>  

  <p>
    The construction reveals an asymmetry between robustness and usefulness. A small component of non-robustness may dramatically increase a feature's vulnarability to attack, but have a minimal impact on its usefulness. Thus, even in situations where there can truly be said to be non-robust features in the data, in the sense of [At odds with accuracy], a bad choice of a feature can still give rise to the creation of non-robust features.
  </p>

  <p>
    We propose that a more intuitive definition of a feature should depend on the data distribution only, and now how it is computed.
  </p>

</d-article>

<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
