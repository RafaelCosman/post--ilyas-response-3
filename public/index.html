<!doctype html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://distill.pub/template.v2.js"></script>
    <style>
        .subgrid {
	grid-column: screen;
	display: grid;
	grid-template-columns: inherit;
	grid-template-rows: inherit;
	grid-column-gap: inherit;
	grid-row-gap: inherit;
}

d-figure.base-grid {
	grid-column: screen;
	background: hsl(0, 0%, 97%);
	padding: 20px 0;
	border-top: 1px solid rgba(0, 0, 0, 0.1);
	border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
	margin-bottom: 1em;
	position: relative;
}

d-figure > figure {
	margin-top: 0;
	margin-bottom: 0;
}

.shaded-figure {
	background-color: hsl(0, 0%, 97%);
	border-top: 1px solid hsla(0, 0%, 0%, 0.1);
	border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
	padding: 30px 0;
}

.pointer {
	position: absolute;
	width: 26px;
	height: 26px;
	top: 26px;
	left: -48px;
}

#rebuttal,
.response-info {
	margin: 1em 0;
	background-color: hsl(228, 50%, 97%);
	border-left: solid hsl(229, 50%, 25%) 3px;
	padding: 1em;
}

#rebuttal,
.rebuttal-info {
	color: hsl(129, 50%, 15%);
	background-color: hsl(128, 50%, 97%);
	border-left: solid hsl(128, 50%, 25%) 3px;
	margin-bottom: 0.5em;
}

#rebuttal figure {
	background: white;
	padding: 1em;
	border-radius: 1em;
}

    </style>
    <style>
        .eq-grid {
            padding: 0px;
            padding-left: 0px;
            display: grid;
            justify-content: center;
            align-items: center;
            grid-row-gap: 0px;
            margin-top: 0px;
            margin-bottom: 10px;
            margin-top: -10px;
        }

        .option {
            font-size: 16px;
        }

        .small {
            font-size: 12px;
            color: grey;
        }
    </style>

</head>

<body>

    <d-front-matter>
        <script type="text/json">{
  "title": "Two Examples of Useful, Non-Robust Features",
  "description": "An example project using webpack and svelte-loader and ejs to inline SVGs",
  "authors": [
    {
      "author": "Gabriel Goh",
      "authorURL": "https://gabgoh.github.io",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
    </d-front-matter>

    <d-title></d-title>

    <d-article>

        <style>
    #rebuttal,
    .comment-info {
        background-color: hsl(54, 78%, 96%);
        border-left: solid hsl(54, 33%, 67%) 1px;
        padding: 1em;
        color: hsla(0, 0%, 0%, 0.67);
    }

    #header-info {
        margin-top: 0;
        margin-bottom: 1.5rem;
        display: grid;
        grid-template-columns: 65px max-content 1fr;
        grid-template-areas:
            "icon explanation explanation"
            "icon back comment";
        grid-column-gap: 1.5em;
    }

    #header-info .icon-multiple-pages {
        grid-area: icon;
        padding: 0.5em;
        content: url(images/multiple-pages.svg);
    }

    #header-info .explanation {
        grid-area: explanation;
        font-size: 85%;
    }

    #header-info .back {
        grid-area: back;
    }

    #header-info .back::before {

        content: "←";
        margin-right: 0.5em;
    }

    #header-info .comment {
        grid-area: comment;
        scroll-behavior: smooth;
    }

    #header-info .comment::before {
        content: "↓";
        margin-right: 0.5em;
    }

    #header-info a.back,
    #header-info a.comment {
        font-size: 80%;
        font-weight: 600;
        border-bottom: none;
        text-transform: uppercase;
        color: #2e6db7;
        display: block;
        margin-top: 0.25em;
        letter-spacing: 0.25px;
    }
</style>

<section id="header-info" class="comment-info">
    <div class="icon-multiple-pages"></div>
    <p class="explanation">
        This article is part of a discussion of Ilyas et al. paper
        <em>"Adversarial examples are not bugs, they are features".</em>
        You can learn more in the
        <a href="/2019/advex-bugs-discussion/">
            main discussion article
        </a>.
    </p>
    <a class="back" href="/2019/advex-bugs-discussion/#comments">Other Comments</a>
    <a class="comment" href="#rebuttal">Comment by Ilyas et al.</a>
</section>


        <p>
            Ilyas et al. <d-cite key="ilyas2019adversarial"></d-cite> define a <i>feature</i> as a function $f$ that
            takes $x$ from the <i>data distribution</i> $(x,y) \sim \mathcal{D}$ into a real number, restricted to have
            mean zero and unit variance. A feature is said to be <i>useful</i> if it has high correlation with the
            label. But in the presence of an adversary Ilyas et al. <d-cite key="ilyas2019adversarial"></d-cite> argues
            the metric that truly matters is a feature's <i>robust usefulness</i>,
        </p>

        <div class="eq-grid">
            <d-math block>
                \mathbf{E}\left[\inf_{\|\delta\|\leq\epsilon}yf(x+\delta)\right],
            </d-math>
        </div>

        <p>
            its correlation with the label while under attack. Ilyas et al. <d-cite key="ilyas2019adversarial"></d-cite>
            suggests that in addition to the pedestrian, robust features we know and love (such as the color of the
            sky), our models may also be taking advantage of useful, non-robust features, some of which may even lie
            beyond the threshold of human intuition. This begs the question: what might such non-robust features look
            like?
        </p>

        <h3>Non-Robust Features in Linear Models</h3>

        <p>

            Our search is simplified when we realize the following: non-robust features are not unique to the complex,
            nonlinear models encountered in deep learning. As Ilyas et al <d-cite key="ilyas2019adversarial"></d-cite>
            observe, they arise even in the humblest of models &#8212; the linear one. Thus, we restrict our attention
            to linear features of the form:

        </p>

        <!--     The search for such a non-robust feature is considerabily simplified when we realize we the phenomenan of non-robust features are not unique to complex, non-linear models. Thus, we restrict our attention to linear features of the form -->
        <div class="eq-grid">
            <d-math block>f(x) = \frac{a^Tx}{\|a\|_\Sigma}\qquad \text{where} \qquad \Sigma = \mathbf{E}[xx^T] \quad
                \text{and} \quad \mathbf{E}[x] = 0.
            </d-math>
        </div>

        <p>
            The robust usefulness of a linear feature admits an elegant decomposition
            <d-footnote> This
                <d-math block>
                    \begin{aligned}
                    \mathbf{E}\left[\inf_{\|\delta\|\leq\epsilon}yf(x+\delta)\right] &
                    =\mathbf{E}\left[yf(x)+\inf_{\|\delta\|\leq\epsilon}yf(\delta)\right]\\
                    & =\mathbf{E}\left[yf(x)+\inf_{\|\delta\|\leq\epsilon}y\frac{a^{T}\delta}{\|a\|_{\Sigma}}\right]\\
                    &
                    =\mathbf{E}\left[yf(x)+\frac{\inf_{\|\delta\|\leq\epsilon}a^{T}\delta}{\|a\|_{\Sigma}}\right]=\mathop{\mathbf{E}[yf(x)]}-\epsilon\frac{\|a\|_{*}}{\|a\|_{\Sigma}}
                    \end{aligned}
                </d-math>
            </d-footnote> into two terms:

        </p>

        <style>
            .undomargin {
                position: relative;
                left: -1em;
                top: 0.2em;
            }
        </style>
        <!-- TODO: rework as CSS grid so smaller screens work, too -->
        <div style="display:flex; flex-direction: column; padding: 0px 80px 0px 80px">

            <div style="display:flex;">

                <div style="margin: auto 0; flex: 3">
                    <span class="undomargin">
                        <d-math block>
                            \mathbf{E}\left[\inf_{\|\delta\|\leq\epsilon}yf(x+\delta)\right]
                        </d-math>
                    </span>
                </div>

                <div style="margin: auto 0; flex: 0.5">
                    <span class="undomargin">
                        <d-math block>=</d-math>
                    </span>
                </div>

                <div style="margin: auto 0; flex: 2">
                    <span class="undomargin">
                        <d-math block>\mathop{\mathbf{E}[yf(x)]}</d-math>
                    </span>
                </div>

                <div style="margin: auto 0; flex: 0.5">
                    <span class="undomargin">
                        <d-math block>-</d-math>
                    </span>
                </div>

                <div style="margin: auto 0; flex: 2">
                    <span class="undomargin">
                        <d-math block>\epsilon\frac{\|a\|_{*}}{\|a\|_{\Sigma}}</d-math>
                    </span>
                </div>

            </div>

            <div style="display:flex; margin-bottom: 30px">

                <div style="line-height: 16px; flex: 3;" class="small">
                    The robust usefulness of a feature
                </div>

                <div style="line-height: 16px; flex: 0.5"></div>

                <div style="line-height: 16px; flex: 2" class="small">
                    the correlation of the feature with the label
                </div>

                <div style="line-height: 16px; flex: 0.5"></div>

                <div style="line-height: 16px; flex: 2" class="small">
                    the feature's non-robustness
                </div>

            </div>
        </div>

        <p>
            In the above equation $\|\cdot\|_*$ deontes the dual norm of $\|\cdot\|$.
            This decomposition gives us an instrument for visualizing any set of linear features $a_i$ in a two
            dimensional plot.
        </p>
        <o>
            Plotted below is the binary classification task of separating <i>truck</i> and <i>frog</i> in CIFAR-10 on
            the set of features $a_i$ corresponding to the $i^{th}$ singular vector of the data.
            </p>

            <figure>
                <div style="position:relative; height:45px"></div>
                <div id="what" style="position:relative; height:505px"></div>
            </figure>

            <p>
                The elusive non-robust useful features, however, seem conspicuously absent in the above plot.
                Fortunately, we can construct such features by strategically combining elements of this basis.
            </p>

            <p>
                We demonstrate two constructions:
            </p>

            </p>

            <figure>
                <div style="position: relative; width:100px; height:840px">
                    <div style="position: absolute; left:0px" id="ensemble"></div>
                    <div style="position: absolute; left:0px" id="distractor"></div>
                </div>
            </figure>

            <p>
                It is surprising, thus, that the experiments of Madry et al. <d-cite key="ilyas2019adversarial">
                </d-cite> (with deterministic perturbations) <i>do</i> distinguish between the non-robust useful
                features generated from ensembles and containments. A succinct definition of a robust feature that peels
                these two worlds apart is yet to exist, and remains an open problem for the machine learning community.
            </p>

            <div class="comment-info">
    To cite Ilyas et al.'s response, please cite their
    <a href="/2019/advex-bugs-discussion/original-authors/#citation">collection of responses</a>.
</div>


            <section id="rebuttal">
                <p><b>Response Summary</b>: The construction of explicit non-robust features is
                    very interesting and makes progress towards the challenge of visualizing some of
                    the useful non-robust features detected by our experiments. We also agree that
                    non-robust features arising as “distractors” is indeed not precluded by our
                    theoretical framework, even if it is precluded by our experiments.
                    This simple theoretical framework sufficed for reasoning about and
                    predicting the outcomes of our experiments
                    <d-footnote>We also presented a theoretical setting where we can
                        analyze things fully rigorously in Section 4 of our paper.</d-footnote>.
                    However, this comment rightly identifies finding a more comprehensive
                    definition of feature as an important future research direction.
                </p>

                <p><b>Response</b>: These experiments (visualizing the robustness and
                    usefulness of different linear features) are very interesting! They both further
                    corroborate the existence of useful, non-robust features and make progress
                    towards visualizing what these non-robust features actually look like. </p>

                <p>We also appreciate the point made by the provided construction of non-robust
                    features (as defined in our theoretical framework) that are combinations of
                    useful+robust and useless+non-robust features. Our theoretical framework indeed
                    enables such a scenario, even if&mdash;as the commenter already notes&mdash;our
                    experimental results do not. (In this sense, the experimental results and our <a
                        href="/2019/advex-bugs-discussion/rebuttal/#takeaway1">
                        main takeaway </a> are actually stronger than our theoretical
                    framework technically captures.) Specifically, in such a scenario, during the
                    construction of the $\widehat{\mathcal{D}}_{det}$ dataset, only the non-robust
                    and useless term of the feature would be flipped. Thus, a classifier trained on
                    such a dataset would associate the predictive robust feature with the
                    <i>wrong</i> label and would thus not generalize on the test set. In contrast,
                    our experiments show that classifiers trained on $\widehat{\mathcal{D}}_{det}$
                    do generalize.</p>

                <p>Overall, our focus while developing our theoretical framework was on
                    enabling us to formally describe and predict the outcomes of our experiments. As
                    the comment points out, putting forth a theoretical framework that captures
                    non-robust features in a very precise way is an important future research
                    direction in itself. </p>
            </section>

            <div class="comment-info">
    You can find more responses in the <a href="/2019/advex-bugs-discussion/"> main discussion article</a>.
</div>


    </d-article>

    <d-appendix>
        <h3>Acknowledgments</h3>
        <p>
            Shan Carter (design overhaul), Preetum (technical discussion), Chris Olah (technical discussion), Ludwig
            (overall feedback), Ria (feedback) Aditiya (feedback)
        </p>

        <h3>Author Contributions</h3>
        <p>
            <b>Research:</b> Alex developed ...
        </p>

        <p>
            <b>Writing & Diagrams:</b> The text was initially drafted by...
        </p>


        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
    </d-appendix>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>
